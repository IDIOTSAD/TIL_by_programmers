# 강의 요약

### Visual Slam introduction
* 컴퓨터 비전은 이미지속에 어떠한 객체가 있는지, 어디에 있는지까지 발전함.
* 목적에 따른 다양한 태스크가 만들어짐. - Classification(객체 검출), Detection(객체 검출, 위치 탐색까지), Segmentation (픽셀이 어떤 객체인지)
* 공통적으로 이미지 속 객체의 생김새와 모양을 학습함. - 해당 정보로 다양한 애플리케이션을 만들 수 있음. (사진 보정, CCTV 사람 검출)
* 이미지 인식이 발전하면서 동영상 인식도 발전하게 됨. - 이미지와 동영상과 다를 게 없음 (프레임을 추출하여 이미지 인식기술을 임시마다 적용)
* 연속된 이미지의 특징(동영상)으로, 다음 영상이 어떠한 영상이 나올 지 예상이 가능해짐. - 주변정보를 이용하여 정확하게 인식하게 됨.
* 컴퓨터 비전은 사진과 영상의 발전이라 해도 과언이 아님.
* 2D 인식부분은 상당부분 발전했지만, 3D을 완벽하게 이해하지 못함. - 3D를 2D 영상으로 이해하는것은 어려움.
* 깊이 정보가 손실되기 때문에 거리를 직접 측정하거나, 기하학적 방법으로 복원했어야 함.
* 데이터가 쌓이면서, 2D -> 3D로 복원된 이미지를 추론할 수 있는 딥러닝 네트워크가 발달함.

* 자율주행에서 사용되는 Perception
* Lane Detection, Object Detection, Segmentation - 이전에 배웠던 내용
* Object tracking, motion prediction - 객체의 움직임을 예측하는 것.
* 3D object detection, pose estimation - 객체를 2D 바운딩 박스를 넘어 방향성까지 파악하는 것. 객체의 거리도 파악함. (3D를 완벽히 이해해야함)

* Visual Slam은 여러장의 사진을 이용하여 3D의 공간을 유추하고, 나의 이동 경로를 유추하는 기술 (공간, 위치, 상태를 이해)
* 자율주행, 자율비행, 메타버스에서 활발히 사용됨.
* 자율주행 - 주행공간 지도를 만들어, 차량의 위치를 파악하는 측위 기술
* 자율비행 - 비행 중 실시간으로 이동이 가능한 위치 파악.
* 메타버스 - 3D 공간을 유추하여 가상 공간의 물체를 현실세계에 소환.
* 딥러닝과 융합할 가능성이 높은 기술임.

* Visual Slam은 딥러닝으로 풀 수 없는 문제를 풀 수 있게 됨.
* 1. 현재 보이는 공간을 넘어서 지금까지 내가 주행했던 3D 공간에 대한 기억을 가지게 됨.
* 공간에 대한 기억은 지도라고 할 수 있음.
* 어떤 공간에서 다른 공간으로 갈 수 있는 방법을 모색하거나, 주행 가능/불가능 한 구간을 파악, 차선을 지도로 파악할 수 있음.
* 매번 추론해서 알아야했던 내용을 추론 없이 바로 알아 낼 수 있음.
* 2. 현재 위치에 대한 정보와 주변 벽과 이동체의 위치 파악 가능.
* 기존의 방식은 다양한 모션 센서를 퓨전하여 추론하여 추론된 위치에 퍼셉션 정보를 추가해 주변을 파악하는 방법.
* 이런 방식은 위치 정보가 흩어지게 되면, 주변 정보 역시 영향을 받아 흩어지게 되는 단점이 있음.
* 딥러닝 정보로 위치를 보정 할 수 없는 단점도 존재함.
* 비쥬얼 슬램은 주변 정보와 위치 정보를 동시에 추론하기 때문에, 상호 보완적으로 정확하게 추론 할 수 있도록 해줌.

### SLAM 개요 (Localization, Mapping, SLAM)
* SLAM
* Simultaneous = 동시적, Localization = 위치추정, And = 및, Mapping = 지도작성
* SLAM은 로봇 기술로 시작하였음. - 동시적 위치 추적 및 지도 작성이 가능해야함.
* 1. 로봇이 어떤 좌표에 있는지 알아야 함. 
* 2. 로봇은 움직이기 때문에 움직임을 추종할 수 있어야 함.

* 지도 - 어떠한 공간을 표현하기 위한 수단 (다양한 방법, 형태를 나타냄)
* 실제 대상을 표현 할 수 있는 수학적 모델 - 모델이라고 함.
* SLAM을 한다면, 어떤 모델로 실제 세상을 표현할 수 있어야하는지?

* 위치 추종과 지도 작성은 어떻게 해야하는가?
* 센서를 통해서 한다. - 로봇의 다양한 감각을 담당함. (카메라, 라이다는 눈, GPS, IMU는 촉각?)
* 눈이 없으면 지도를 볼 수 없듯이 로봇도 눈이 없으면 볼 수 없음. - 지도도 만들 수 없음.
* 사전 정보가 없을 때 위치 추종과 지도 작성을 해야하는 것은 사전에 계산해둔 위치 정보가 있으면 현재 위치에서 사전 정보를 이용하여 정확한 위치 추종가능.
* SLAM은 사전 정보가 아예 없음 - 없어도 풀 수 있는 것. (무인도에서 눈을 딱 떴을때.. )

* 만약 사전 정보가 있다면?
* 사전정보가 없이 계산하기엔 미지수가 너무 많기 때문에 파란 점이 많아짐.
* 사전정보가 있으면 대략적인 정보를 얻을 수 있음 - 과거의 정보이고, 완벽한 정보는 아님. 현재위치에서 특정 위치에 대한 분포를 가지게 됨.
* 적절한 좋은 사전정보를 담은 이전 정보가 있다면 좋지만, SLAM은 없어도 추종가능함.

* SLAM은 동시적으로 위치추정과 지도작성을 하겠다는 것.
* 위치추정과 지도작성을 동시적으로 하는 것과 차이점? or 왜 다른 기술이 아니라 위치추정과 지도 작성을 섞는가? (다양한 디텍션, 세그멘테이션을 섞을 수도 있는데..)
* 다른 위치추정 기술과의 차이점은? 다른 지도작성 기술과의 차이점? - 왜 사전 정보를 안쓰는거지? (어려운 조건으로)
* 왜 SLAM 기술이 특별한지 이해 할 수 없음. - 정확하게 이해할 필요가 있음.

* SLAM은 Mobile Robotics에서 시작함. (이동 가능한 로보틱스)
* 사람이 직접 가기 힘든 곳을 대신 탐색 - 해저, 탄광, 화산, 원자력발전기, 분진, 전쟁 등 (위험한 곳) | 교량 하부, 풍력발전기 (사람이 가기 어려움)
* 이동 자체를 자동화 - 비용을 비교 (사람 vs 로봇) 인건비, Time to failure / Safety | 운전을 자동화
* 고정된 로봇의 workspace 확장 (가동되는 로봇의 움직임을 확대) - 바퀴 / 캐터필터 / 레일 부착
* 자율이동체의 조건 - 로봇의 조건 : 인지, 결정, 행동 (로봇이 움직이기만 하는건 단순 반복작업에 불과)


* 이동체에게 주변 공간을 인지하는 것은?
* 1. 이동 가능한 지역과 불가능한 지역 파악
* 2. 벽이나 장애물을 감지 가능
* 가장 유명한 센서는 라이다. - 이런 장비를 Exteroceptive sensing이라고 함. (Extero~ = 외부, ~ceptive = 감각)
* 산업용 로봇은 인지보다 제어가 중요 (정밀한 제어)
* 세상에 완벽한 센서는 없음 (역할은 실제 세상에 있는 물리적인 정보를 수치화 하는 것)
* 수치화하는데 있어서 오차가 생길 수 밖에 없음 - 이를 노이즈라고 함. 이는 센서의 한계로 인해 생김.
* SLAM은 확률적인 프로세스임.

* 자기 자신의 움직임을 인식하는 센서가 있어야 벽, 장애물이나 이동이 가능해지게 됨.
* 대표적인 센서는 GPS, IMU - 이런 장비를 Proprioceptive sensing라고 함. (Pro~ = 나 자신, ~captive = 감각, 인지)

* 센서의 값을 읽고(이동 값 파악 + 벽 위치 파악) 이동하여 다시 값을 읽는 반복적인 작업을 수행하게 됨.
* 이를 Perception & Control feedback loop 라고 함.
* Proprioceptive sensing의 출력값이 내 위치를 인지해줌. - Exteroceptive sensing의 출력값이 주변 환경을 인지해줌.
* 센서의 종합값이 확률적으로 분포하기 때문에, 해당 값들의 평균값, 중간값 등 다양한 방법을 통해 제일 좋은 값을 얻어냄.
* 최적의 값을 얻기 위해서는 여러개의 센서를 사용해야하는 점과, 최적의 값을 얻기 위해서는 잠시 멈춰있어야 하는 점이 있음.

* 이 과정에서 1m를 움직였는데 센서에서는 0.8m 움직였다고 하면, 값의 오차가 크다고 볼 수 있음 - 1m의 정보를 모르게 되면 0.8m가 움직였다고 알 수 없음.
* 하나의 센서에서는 여러개의 값을 얻어 낼 수 없기 때문이다. - 이미 센서에서 얻을 수 있는 값을 다 얻어낸 경우임. - 이로 인해 분포를 내어 최적의 값을 낼 수 없음.

* 이를 해결하기 위해서는 여러 개의 센서를 사용하는 방법이 있음.
* 하지만, 여러개의 센서 탑재로 비싸짐. 그리고 어떠한 센서가 정확한지 알 수 없음.
* 두 번째 방법으로는 Exteroceptive sensing의 데이터를 다시 봄으로써 Proprioceptive sensing의 오차를 찾아내는 방법이 있음.
* 하지만, 두 센서가 동일한 오차를 가지고 있을 때 정확한 값을 얻을 수 없음. 유리를 보거나 하는 것으로 오차가 뻥튀기 되는 경우.
* Exteroceptive sensing을 샘플링하는 동안에는 멈춰야하므로 속도가 매우 느림.
* 이러한 두 방법을 보완하기 위해서는 완벽한 모바일 로보틱스는 각 센서의 오차가 적어야하고(안정적인 값), 지속적으로 움직이면서 모션과 주변환경 인지 할 수 있어야 함.

* 모든센서는 확률적으로 표현 가능함. 센서의 종류는 Exteroceptive sensing(주변환경 인지), Proprioceptive sensing(나 자신의 움직임 인지)
* 두 센서는 안정적인 값을 도출 할 수 있어야 함. - 각 센서는 확률적인 분포를 가지고 있으며, 이 둘간의 상관관계가 있다.
* 확률 분포를 조합할 때는 조심해야함. - 두 센서가 불안정하면 둘 다 보정할 순 없지만, 하나의 센서만 불안정적이면 보정이 가능하다는 점을 이용한다.
* 낮은 정확도를 가지는 Exteroceptive sensing, 높은 정확도를 가지는 Proprioceptive sensing -> Mapping
* 높은 정확도를 가지는 Exteroceptive sensing, 낮은 정확도를 가지는 Proprioceptive sensing -> Localization

* 대동여지도를 보자. - 대동여지도는 어떻게 만들어졌을까?
* 사람이 직접 움직여서 지도를 그리고, 움직이고 반복하면서 작은 지도에서 크게 키움.
* 움직이면서 보면 하나의 물체를 자세하게 볼 순 없지만, 다양한 자세에서 볼 수 있게 됨.
* 나 자신의 위치를 정확하게 알고 있을 때, 불안정한 주변환경을 인지하여 보정 했을 것이다.
* (낮은 정확도를 가지는 Exteroceptive sensing, 높은 정확도를 가지는 Proprioceptive sensing)

* 롯데월드 안내지도를 보자.
* 친구가 남문 입구로 오라 했을 때, 지도를 보고 남문 입구로 갈 수 있게 됨.
* 정확한 지도를 가지고 있고, 부정확한 내 자신의 위치를 가지고 있어서 추론 할 수 있게 됨.
* (높은 정확도를 가지는 Exteroceptive sensing, 낮은 정확도를 가지는 Proprioceptive sensing)

* 자동차 차선을 보자.
* 내가 앞으로 갈 공간을 사전에 파악. 이에 맞춰서 경로를 미리 맞출 수 있음.
* 갈 수 있는 곳과 없는 곳을 지도를 통해 분류할 수 있음. - 차선의 경계, 신호등, 표지판 등의 정보도 있음.
* 정확하게 그려진 차선, 신호등, 표지판이 있어도 내 자신(차량)이 비틀거리며 운행한다면? - 사고가 날 수 있음.
* 어떤 차선에 있는지 정확하게 알아야 경로를 분류할 수 있게 됨.
* 정확한 위치정보를 힌트삼아서 지도를 잘 만드는 기술 - 맵핑
* 정확한 지도를 힌트삼아서 위치를 잘 추정하는 기술 - 로컬라이제이션

* 주어진 힌트로 인해서 제약조건이 생김. - 힌트가 없을때!
* Monte Carlo localization - 지도가 사전정보가 주어졌을 때, 파티클 필터를 사용하여 위치를 추종함
* 파티클 필터 - 모션정보와 옵져베이션(관찰) 정보 (Pro, Ext 센서의 확률분포를 조합해서 최적의 정보(Pose, 위치정보)를 갖는 것)
* 1. initialization 단계 - configration space에 파티클을 쫙 뿌림. (파티클은 로봇이 위치 할 수 있는 정보)
* 2. motion update 단계 - 뿌려진 파티클마다, Pro 센서로부터 들어온 모션 정보를 추가하여 위치 정보를 업데이트 - 파티클이 벽에 들어가거나 존재할 수 없는 위치는 삭제
* 3. measurement 단계 - 뿌려진 파티클 마다, Ext 센서로부터 들어온 옵져베이션 정보를 덧씌움.
* 4. weight update 단계 - 해당 위치에 실제 옵져베이션 값이 나올 수 있는지 파티클의 현재 위치 정보와 주변환경 정보가 맞는지 계산을 수행
* 5. resampling 단계 - 현재 위치와 주변 환경정보가 맞아 떨어지는 파티클만 남기고 주변에서 파티클을 새로 뿌려서 재작업.
* 지도속에서 어디에 로봇이 존재해야 위치와 환경정보가 맞아떨어지는지 찾는 과정을 누적하면서 정답에 가까워지는 것이 몬테칼로 로컬라이제이션의 핵심.
* 비슷한 공간이 있을 때, 위치를 헷갈릴 수 있다 - 이는 몬테칼로 알고리즘의 특징. 또한, 몬테칼로는 지도를 전적으로 믿는다는 점을 알 수 있다.

* 닭이 먼저인가? 알이 먼저일까? - 로컬라이제이션이 먼저되어야 하는가? 맵핑이 먼저 되어야하는가? 문제
* 예전에는 좋은 센서를 통해서 해결 하였음. - 가격적인 문제가 있음. 그리고 제약조건이 있었음 (실내, 실외)
* 완전히 새로운 환경에서는 어떻게 풀어야 했을까? - 원래는 문제를 풀 수 없었음. 이를 해결하기 위해 SLAM이 나옴.
* SLAM은 아무 사전 정보 없이 Pro, Ext 정보를 동시에 받아 동시에 추정하는 것

* SLAM은 위치 정보나 지도 정보가 아예 없거나 불완전할때 사용가능
* 최적의 위치, 지도 정보를 추론 가능하고, 아무런 사전정보가 없어도 됨.
* Localization은 높은 수준의 지도가 있을 때, 위치를 추론하는 방법
* Mapping은 높은 수준의 위치정보가 있을 때, 지도를 추론하는 방법
* 가장 중요한 것은 높은수준의 지도와 위치정보가 있으면 만사형통. - SLAM도 안써도 됨.

### SLAM의 종류
* SLAM에서 사용 할 수 있는 센서
* 지난 시간에 Pro, Ext 센서를 배웠음.

* Proprioceptive sensor의 종류 - IMU, Wheel encoder

* Wheel encoder - 바퀴의 회전량(RPM), 이동량(=바퀴의 회전량 * 바퀴의 둘레)을 측정하는 센서
* 브러시 인코더나, 빛을 이용한 옵티컬 센서, 자기장, 전기를 이용한 마그네틱, 인덕티브 캐패시브 센서를 사용
* 이를 이용한 알고리즘 - 데드레커닝 기법. - 모션값을 누적해가면서 추정함. 
* 장점 - 자동차에는 기본적으로 탑재 - 로봇, 드론에도 포함되어 있음.
* 단점 - 시간이 오래지날수록 에러가 누적되는 단점, 바퀴가 헛도는 경우 잘못된 센서의 값이 생길 수 있음(비가 오거나 눈이 오는 경우).
* 단점 - 바퀴의 둘레가 주행중 너무 자주 바뀜 (탑승자의 무게, 코너링, 바람빠짐, 마찰열로 인한 타이어팽창)

* IMU - 선형가속도(Linear accelerator)를 측정하는 센서와 각속도(Angular gyroscope)를 측정하는 센서가 혼합된 센서
* Spring-damper system의 원리를 이용함
* Optical system - 차량용 IMU
* MEMS - 스마트폰 및 소형 디바이스 IMU
* 장점 - Consumer grade 제품은 저렴한 편 (자동차는 동일 성능에 저렴하지 않음), 높은 sensitivity, 높은 fps(100hz ~ 4000hz)
* 단점 - 엄청나게 빠른 drift 누적 - 보정을 위해 Camera, LiDAR, GNSS와 함께 사용
* 비전과 융합한 VIO,VINS 라이다와 결합한 LIO, LINS가 유행하고 있음 - IMU를 통해 얻은 프라이오 정보를 다른 센서와 결합해서 좀 더 빠르게 얻기 위함.

* Exteroceptive sensors의 종류 - GNSS, LiDAR, Camera, Ultrasound, RADAR, Microphone

* GNSS - Global navigation satellite system (GPS는 미국의 GNSS 시스템임.)
* 비콘 기반의 위치 추정 센서 (위성)
* 다수의 비콘에 대한 통신시간 차이를 이용하여 비콘-로봇의 거리를 구하고, 삼각 측량을 통해 localization 수행
* Ego-motion을 추정하기 때문에, proprioceptive sensor 같지만, 외부 비콘을 이용하기 때문에 exteroceptive sensor임.
* 나라마다 시스템이 다름. (GPS-USA, GLONASS-Russia, BeiDou-China, Gallileo-Europe, KPS-Korea)
* 장점 - 싸고 사용하기 쉬움.
* 단점 - 부정확함 (10 - 20m 오차), RTK-GPS, DGPS를 사용할 경우 오차는 cm 단위로 내려옴 (가격이 억대)
* 단점 - 고층 빌딩 사이에 multi-path 문제, 실내/지하 사용 불가능, KPS가 아직 없음.

* LiDAR - Light detection and ranging sensors
* 적외선 레이저를 쏘고 반사 시간을 측정하여 거리를 추정하는 센서
* Time-of-Flight(ToF), Phase shift, Frequency modulation 레이저 방식
* Mechanical scanner, Solid state scanner, Flash LiDAR 스캐닝 방식
* 주변 환경을 3D Point Cloud 형태로 바로 알 수 있음.
* 장점 - Exteroceptive 센서 중 가장 정확한 편, 자율주행용 라이다는 ~100m 유효거리, 빛의 파장이 일어나지 않기 떄문에 낮/밤 사용가능(동일한 device로 인한 간섭 제외)
* 단점 - 비쌈(16ch - 500~600만원, 128ch - n천만원), 카메라에 비해 해상도가 낮음, 눈/비/안개 영향을 받음, Multi-path 문제, Solid-state LiDAR의 경우 여러 방향으로 탑재 필요

* RADAR - Radio detection and ranging sensor
* 반사되어 돌아오는 전파를 측정하여 radial 거리를 재는 센서
* Doppler 효과를 이용해서 이동중인 물체의 속도 추정 가능
* 전파의 종류를 바꿈으로써 nerar-range와 far-range 선택 가능
* 장점 - 날씨 영향X, 타 센서에서는 얻지 못하는 속도 값 추정 가능
* 단점 - 작은 물체들은 detection 실패, LiDAR 보다 낮은 해상도, Multi-path 문제
* Tesla는 RADAR를 버렸다?
* RADAR가 안좋은게 아니라, Tesla의 비전 시스템이 RADAR 성능을 상회했기 때문에 RADAR 사용 중지
* Tesla는 딥러닝 특화 자율주행 컴퓨터가 탑재 되었기 때문임.

* Ultrasonic
* 초음파를 이용 - RADAR와 작동 방식 동일
* 장점 - 저렴함, Near-range에서 잘 작동함.
* 단점 - 물체의 형태를 잘 추정하지 못함. 그래서 많이 Distance 센서로 사용함, 노이즈가 많음.

* Camera
* 광센서를 이용해 빛 신호를 받고, debayering 프로세스를 통해 RGB 색 재구성
* 장점 - 저렴함, 좋은 성능(dense data, texture, color, high-fps) 렌즈 교환을 통해 시야각 변경 가능, 사람이 보는 시야와 가장 유사함. (시각화하기 좋은 자료)
* 단점 - depth 정보 손실, 조명 영향

* Microphone
* 공기의 진동을 transducer 센서를 통해 전기 신호로 변환하는 센서
* 여러개의 마이크를 통해 소리의 근원에 대한 위치를 계산 가능
* 장점 - 유일하게 소리 정보를 사용하는 센서, 저렴한 가격
* 단점 - Geometry가 부정확함, 잡음이 심함, 고객이 없음.

### SLAM의 종류
* Visual-SLAM
* Visual 정보(이미지)를 이용하는 SLAM - Exteroceptive sensor = 카메라

* LiDAR SLAM
* LiDAR 정보를 이용하는 SLAM - Exteroceptive sensor = 라이다

* RADAR SLAM
* RADAR 정보를 이용하는 SLAM - Exteroceptive sensor = 레이다

* Visual-SLAM
* Visual 정보(이미지)를 이용하는 SLAM - Exteroceptive sensor = 카메라
* 장점 - 저렴한 센서를 사용, 센서의 성능을 조절하기 쉬움 (렌즈교체 - 시야각, 초점 조절, 노출 시간)
* 장점 - 센서의 속도가 빠름, 이미지 기반 딥러닝 적용 가능, 이미지로 사람이 이해하기 쉬운 시각화 가능
* 단점 - 갑작스러운 빛 변화에 대응 불가능, 시야가 가려지거나 어두운 곳에서 사용 불가능

* Camera = Camera device + Lens
* 센서가 어떠한 노이즈를 가지고 있는가를 측정해야함.
* 3D 이미지를 2D 이미지로 변환하기 떄문에, 이 과정에서 생기는 한계점에서 노이즈가 나타남 (이미지 센서 - 아날로그를 디지털로 변환 하는 과정, 렌즈 - 차원의 변화에서 나타나는 오류 등)
* Monocular camera - 1대 카메라
* Stereo camera - 2대 카매라 / Multi camera - N개 카메라
* RGB-D Camera (depth camera)를 사용

* Monocular camera
* 특징 - 1대의 카메라에서만 이미지를 받음 (연구용 알고리즘이라는 인식이 있음)
* 장점 - Stero, Multi camera VSLAM보다 저렴함 (센서 가격, 전력소비량, 이미지 데이터 송수신 대역폭)
* 단점 - Scale ambiguity - 3D 공간을 실제 스케일로 추정할 수 없음. (up-to-scale로만 추정가능)
* 이문제를 풀기 위해서는 metric scale을 가진 proprioceptive sensor가 필요함.
* Metric scale : 실제 세상에서 통용되는 미터 단위 - metric의 스케일
* 최근 딥러닝 기반 monocular depth estimation으로 문제를 해결하려는 시도가 있음.
* 이전에 촬영 하였던 카메라 데이터와 현재 데이터를 이용하여 3D를 추종함. - 그리고, 명암의 차이가 큰 부분을 포인트로 삼음.
* 이러한 Monocular로 진행하는 SLAM을 sparse(depth) SLAM이라고 함. - dense SLAM과 비교해서 계산량이 적음.

* Stero / Multi camera VSLAM
* 특징 - Stero : 2대의 카메라를 사용함, Multi : N대의 카메라를 이용함.
* 특징 - 인접한 카메라들 간의 baseline 거리를 이용하여 삼각측량을 통해 거리/깊이 측량 가능.
* 장점 - 두 이미지 간의 displarity 정보를 이용하여 픽셀마다 depth를 추정할 수 있음. Metric scale의 3D 공간을 복원가능
* 단점 - 카메라 설정 및 캘리브레이션이 어려움.
* * 모든 카메라는 동시에 이미지를 취득해야함 (synchronized cameras)
* * Baseline이 충분히 길어야 먼 거리의 3D 공간을 정확하게 측량할 수 있음.
* * 카메라들마다 Intrinsic / Extrinsic 캘리브레이션을 정확하게 해야함. 이 과정이 불가능하기도 함.
* 단점 - 모든 픽셀마다 disparity 정보로 depth를 계산하는데에는 많은 계산량이 필요하며, 이를 위해 GPU나 FPGA 계산을 요구하기도 함.

![image](https://user-images.githubusercontent.com/55529455/169829310-999c3eb5-170d-4482-9446-5ba08c7b22ea.png)

* RGB-D VSLAM
* 특징 - Structured light (구조광), 또는 Time-of-Flight (ToF)센서를 이용한 카메라를 사용
* * 센서가 Depth 값을 직접 얻어주기 때문에 계산이 필요하지 않음.
* 특징 - Dense mapping을 많이 하는 편
* 장점 - Depth 데이터를 통해 3D 공간을 metric scale로 실시간 복원 가능
* 단점 - ~10m 정도에서만 depth 데이터가 정확함. Filed of view가 적음. 실외에서 사용불가 (적외선 파장이 햇빛과 간섭)

### SLAM 기술의 적용
* SLAM에서 가장 상업화에 성공한 로보틱스 - 로봇 청소기
* (천장을 보면서 위치를 추종, 초음파 센서로 장애물 감지) - 지도 정보가 없기 때문에, 처음보는 장소에서 움직이며 맵을 그림 (그러다보니 청소가 됨)
* Active SLAM - SLAM + Path Planning을 동시에 풀려고 하는 것.

* 산업 현장 측량 및 스캐너 로봇 / 모니터 로봇
* 벽과 바닥이 평평한지, 위험한 곳을 탐지하기 위해서 사용됨. 삼각대와 측량기를 이용해서 할 필요 없이 로봇이 매핑하여 값을 구함.
* 인건비와 사람의 실수와 수고가 덜어짐. - 이 로봇을 제어하는 원격 전문가가 생길 수도 있음.

* 배달 / 믈류 로봇
* 건설 로봇
* 서비스 로봇 (무빙 키오스크, 캐리어 로봇)
* 위험지대 탐사 로봇

* 자동차 (드라이빙, 자동 주차)

* 드론 (구조 검사 / 스캐너, 액션캠 / 촬영용 드론)

* VR / AR / XR / MR (가상 컨텐츠, 산업용 증강현실, 의료진단 / 원격 의료 / 트레이닝)

### SLAM 특성
* 자율주행 로보틱스
* 제품화를 위한 규제, 자유로운 알고리즘 개발, 하지만 전세계적으로 경쟁
* 이 분야는 이기는 기업이 시장을 선점할 가능성이 높음.
* SW 뿐만 아니라 HW 부분도 매우 중요함 - 제조 단가 (중국과의 경쟁)

* 자율주행 자동차
* 안전이 가장 중요함. - 삐끗하면 인명사고가 날 수 있음.
* 높은 수준의 딥러닝 솔루션 존재, 딥러닝 + SLAM, 안전한 SW
* 오픈소스의 라이브러리들이 안정적으로 잘 작동하는지? - 믿을 수 없으면 직접 알고리즘을 작성해야함.
* 비가와서 카메라에 물 묻은 경우, 주행 중 돌이 튀어서 위치 추정이 불안정, 정전 이슈로 카메라가 꺼진다면? - 안전하게 수리를 유도 할 수 있어야함.
* 안전한 소프트웨어를 만들어야 함.

* 자율비행 드론
* 제품화를 위한 규제, 가장 빠른 SLAM을 요구함.
* 전력을 많이 소모하기 때문에 컴퓨팅에 전력이 제한이 됨. - 가벼운 SLAM 알고리즘을 쓰게 됨.
* 아주 빠르게 정보가 갱신되어야 하기 때문에 최적화가 중요함.
* 안정성도 아주 중요함 - 삐끗하면 드론이 폭발함.

* 메타버스 - VR / AR
* 가장 자유로운 개발 규제, 빠르고, 정확하고, 가벼운 SLAM 알고리즘
* GPU 병렬처리 기술을 이용함. - 실감나는 그래픽 랜더링을 위해서 SLAM은 GPU 자원을 사용 할 수 없을 가능성이 높음.
* SLAM의 부정확성으로 그래픽의 위치 오류로 인해 사용자에게 불만을 야기할 수 있음.






