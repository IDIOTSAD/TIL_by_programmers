# 강의 요약

### Visual Slam introduction
* 컴퓨터 비전은 이미지속에 어떠한 객체가 있는지, 어디에 있는지까지 발전함.
* 목적에 따른 다양한 태스크가 만들어짐. - Classification(객체 검출), Detection(객체 검출, 위치 탐색까지), Segmentation (픽셀이 어떤 객체인지)
* 공통적으로 이미지 속 객체의 생김새와 모양을 학습함. - 해당 정보로 다양한 애플리케이션을 만들 수 있음. (사진 보정, CCTV 사람 검출)
* 이미지 인식이 발전하면서 동영상 인식도 발전하게 됨. - 이미지와 동영상과 다를 게 없음 (프레임을 추출하여 이미지 인식기술을 임시마다 적용)
* 연속된 이미지의 특징(동영상)으로, 다음 영상이 어떠한 영상이 나올 지 예상이 가능해짐. - 주변정보를 이용하여 정확하게 인식하게 됨.
* 컴퓨터 비전은 사진과 영상의 발전이라 해도 과언이 아님.
* 2D 인식부분은 상당부분 발전했지만, 3D을 완벽하게 이해하지 못함. - 3D를 2D 영상으로 이해하는것은 어려움.
* 깊이 정보가 손실되기 때문에 거리를 직접 측정하거나, 기하학적 방법으로 복원했어야 함.
* 데이터가 쌓이면서, 2D -> 3D로 복원된 이미지를 추론할 수 있는 딥러닝 네트워크가 발달함.


* 자율주행에서 사용되는 Perception
* Lane Detection, Object Detection, Segmentation - 이전에 배웠던 내용
* Object tracking, motion prediction - 객체의 움직임을 예측하는 것.
* 3D object detection, pose estimation - 객체를 2D 바운딩 박스를 넘어 방향성까지 파악하는 것. 객체의 거리도 파악함. (3D를 완벽히 이해해야함)


* Visual Slam은 여러장의 사진을 이용하여 3D의 공간을 유추하고, 나의 이동 경로를 유추하는 기술 (공간, 위치, 상태를 이해)
* 자율주행, 자율비행, 메타버스에서 활발히 사용됨.
* 자율주행 - 주행공간 지도를 만들어, 차량의 위치를 파악하는 측위 기술
* 자율비행 - 비행 중 실시간으로 이동이 가능한 위치 파악.
* 메타버스 - 3D 공간을 유추하여 가상 공간의 물체를 현실세계에 소환.
* 딥러닝과 융합할 가능성이 높은 기술임.


* Visual Slam은 딥러닝으로 풀 수 없는 문제를 풀 수 있게 됨.
* 1. 현재 보이는 공간을 넘어서 지금까지 내가 주행했던 3D 공간에 대한 기억을 가지게 됨.
* 공간에 대한 기억은 지도라고 할 수 있음.
* 어떤 공간에서 다른 공간으로 갈 수 있는 방법을 모색하거나, 주행 가능/불가능 한 구간을 파악, 차선을 지도로 파악할 수 있음.
* 매번 추론해서 알아야했던 내용을 추론 없이 바로 알아 낼 수 있음.
* 2. 현재 위치에 대한 정보와 주변 벽과 이동체의 위치 파악 가능.
* 기존의 방식은 다양한 모션 센서를 퓨전하여 추론하여 추론된 위치에 퍼셉션 정보를 추가해 주변을 파악하는 방법.
* 이런 방식은 위치 정보가 흩어지게 되면, 주변 정보 역시 영향을 받아 흩어지게 되는 단점이 있음.
* 딥러닝 정보로 위치를 보정 할 수 없는 단점도 존재함.
* 비쥬얼 슬램은 주변 정보와 위치 정보를 동시에 추론하기 때문에, 상호 보완적으로 정확하게 추론 할 수 있도록 해줌.

### SLAM 개요 (Localization, Mapping, SLAM)
* SLAM
* Simultaneous = 동시적, Localization = 위치추정, And = 및, Mapping = 지도작성
* SLAM은 로봇 기술로 시작하였음. - 동시적 위치 추적 및 지도 작성이 가능해야함.
* 1. 로봇이 어떤 좌표에 있는지 알아야 함. 
* 2. 로봇은 움직이기 때문에 움직임을 추종할 수 있어야 함.

* 지도 - 어떠한 공간을 표현하기 위한 수단 (다양한 방법, 형태를 나타냄)
* 실제 대상을 표현 할 수 있는 수학적 모델 - 모델이라고 함.
* SLAM을 한다면, 어떤 모델로 실제 세상을 표현할 수 있어야하는지?

* 위치 추종과 지도 작성은 어떻게 해야하는가?
* 센서를 통해서 한다. - 로봇의 다양한 감각을 담당함. (카메라, 라이다는 눈, GPS, IMU는 촉각?)
* 눈이 없으면 지도를 볼 수 없듯이 로봇도 눈이 없으면 볼 수 없음. - 지도도 만들 수 없음.
* 사전 정보가 없을 때 위치 추종과 지도 작성을 해야하는 것은 사전에 계산해둔 위치 정보가 있으면 현재 위치에서 사전 정보를 이용하여 정확한 위치 추종가능.
* SLAM은 사전 정보가 아예 없음 - 없어도 풀 수 있는 것. (무인도에서 눈을 딱 떴을때.. )

* 만약 사전 정보가 있다면?
* 사전정보가 없이 계산하기엔 미지수가 너무 많기 때문에 파란 점이 많아짐.
* 사전정보가 있으면 대략적인 정보를 얻을 수 있음 - 과거의 정보이고, 완벽한 정보는 아님. 현재위치에서 특정 위치에 대한 분포를 가지게 됨.
* 적절한 좋은 사전정보를 담은 이전 정보가 있다면 좋지만, SLAM은 없어도 추종가능함.

* SLAM은 동시적으로 위치추정과 지도작성을 하겠다는 것.
* 위치추정과 지도작성을 동시적으로 하는 것과 차이점? or 왜 다른 기술이 아니라 위치추정과 지도 작성을 섞는가? (다양한 디텍션, 세그멘테이션을 섞을 수도 있는데..)
* 다른 위치추정 기술과의 차이점은? 다른 지도작성 기술과의 차이점? - 왜 사전 정보를 안쓰는거지? (어려운 조건으로)
* 왜 SLAM 기술이 특별한지 이해 할 수 없음. - 정확하게 이해할 필요가 있음.

* SLAM은 Mobile Robotics에서 시작함. (이동 가능한 로보틱스)
* 사람이 직접 가기 힘든 곳을 대신 탐색 - 해저, 탄광, 화산, 원자력발전기, 분진, 전쟁 등 (위험한 곳) | 교량 하부, 풍력발전기 (사람이 가기 어려움)
* 이동 자체를 자동화 - 비용을 비교 (사람 vs 로봇) 인건비, Time to failure / Safety | 운전을 자동화
* 고정된 로봇의 workspace 확장 (가동되는 로봇의 움직임을 확대) - 바퀴 / 캐터필터 / 레일 부착
* 자율이동체의 조건 - 로봇의 조건 : 인지, 결정, 행동 (로봇이 움직이기만 하는건 단순 반복작업에 불과)


* 이동체에게 주변 공간을 인지하는 것은?
* 1. 이동 가능한 지역과 불가능한 지역 파악
* 2. 벽이나 장애물을 감지 가능
* 가장 유명한 센서는 라이다. - 이런 장비를 Exteroceptive sensing이라고 함. (Extero~ = 외부, ~ceptive = 감각)
* 산업용 로봇은 인지보다 제어가 중요 (정밀한 제어)
* 세상에 완벽한 센서는 없음 (역할은 실제 세상에 있는 물리적인 정보를 수치화 하는 것)
* 수치화하는데 있어서 오차가 생길 수 밖에 없음 - 이를 노이즈라고 함. 이는 센서의 한계로 인해 생김.
* SLAM은 확률적인 프로세스임.

* 자기 자신의 움직임을 인식하는 센서가 있어야 벽, 장애물이나 이동이 가능해지게 됨.
* 대표적인 센서는 GPS, IMU - 이런 장비를 Proprioceptive sensing라고 함. (Pro~ = 나 자신, ~captive = 감각, 인지)

* 센서의 값을 읽고(이동 값 파악 + 벽 위치 파악) 이동하여 다시 값을 읽는 반복적인 작업을 수행하게 됨.
* 이를 Perception & Control feedback loop 라고 함.
* Proprioceptive sensing의 출력값이 내 위치를 인지해줌. - Exteroceptive sensing의 출력값이 주변 환경을 인지해줌.
* 센서의 종합값이 확률적으로 분포하기 때문에, 해당 값들의 평균값, 중간값 등 다양한 방법을 통해 제일 좋은 값을 얻어냄.
* 최적의 값을 얻기 위해서는 여러개의 센서를 사용해야하는 점과, 최적의 값을 얻기 위해서는 잠시 멈춰있어야 하는 점이 있음.

* 이 과정에서 1m를 움직였는데 센서에서는 0.8m 움직였다고 하면, 값의 오차가 크다고 볼 수 있음 - 1m의 정보를 모르게 되면 0.8m가 움직였다고 알 수 없음.
* 하나의 센서에서는 여러개의 값을 얻어 낼 수 없기 때문이다. - 이미 센서에서 얻을 수 있는 값을 다 얻어낸 경우임. - 이로 인해 분포를 내어 최적의 값을 낼 수 없음.

* 이를 해결하기 위해서는 여러 개의 센서를 사용하는 방법이 있음.
* 하지만, 여러개의 센서 탑재로 비싸짐. 그리고 어떠한 센서가 정확한지 알 수 없음.
* 두 번째 방법으로는 Exteroceptive sensing의 데이터를 다시 봄으로써 Proprioceptive sensing의 오차를 찾아내는 방법이 있음.
* 하지만, 두 센서가 동일한 오차를 가지고 있을 때 정확한 값을 얻을 수 없음. 유리를 보거나 하는 것으로 오차가 뻥튀기 되는 경우.
* Exteroceptive sensing을 샘플링하는 동안에는 멈춰야하므로 속도가 매우 느림.
* 이러한 두 방법을 보완하기 위해서는 완벽한 모바일 로보틱스는 각 센서의 오차가 적어야하고(안정적인 값), 지속적으로 움직이면서 모션과 주변환경 인지 할 수 있어야 함.

* 모든센서는 확률적으로 표현 가능함. 센서의 종류는 Exteroceptive sensing(주변환경 인지), Proprioceptive sensing(나 자신의 움직임 인지)
* 두 센서는 안정적인 값을 도출 할 수 있어야 함. - 각 센서는 확률적인 분포를 가지고 있으며, 이 둘간의 상관관계가 있다.
* 확률 분포를 조합할 때는 조심해야함. - 두 센서가 불안정하면 둘 다 보정할 순 없지만, 하나의 센서만 불안정적이면 보정이 가능하다는 점을 이용한다.
* 낮은 정확도를 가지는 Exteroceptive sensing, 높은 정확도를 가지는 Proprioceptive sensing -> Mapping
* 높은 정확도를 가지는 Exteroceptive sensing, 낮은 정확도를 가지는 Proprioceptive sensing -> Localization

* 대동여지도를 보자. - 대동여지도는 어떻게 만들어졌을까?
* 사람이 직접 움직여서 지도를 그리고, 움직이고 반복하면서 작은 지도에서 크게 키움.
* 움직이면서 보면 하나의 물체를 자세하게 볼 순 없지만, 다양한 자세에서 볼 수 있게 됨.
* 나 자신의 위치를 정확하게 알고 있을 때, 불안정한 주변환경을 인지하여 보정 했을 것이다.
* (낮은 정확도를 가지는 Exteroceptive sensing, 높은 정확도를 가지는 Proprioceptive sensing)

* 롯데월드 안내지도를 보자.
* 친구가 남문 입구로 오라 했을 때, 지도를 보고 남문 입구로 갈 수 있게 됨.
* 정확한 지도를 가지고 있고, 부정확한 내 자신의 위치를 가지고 있어서 추론 할 수 있게 됨.
* (높은 정확도를 가지는 Exteroceptive sensing, 낮은 정확도를 가지는 Proprioceptive sensing)

* 자동차 차선을 보자.
* 내가 앞으로 갈 공간을 사전에 파악. 이에 맞춰서 경로를 미리 맞출 수 있음.
* 갈 수 있는 곳과 없는 곳을 지도를 통해 분류할 수 있음. - 차선의 경계, 신호등, 표지판 등의 정보도 있음.
* 정확하게 그려진 차선, 신호등, 표지판이 있어도 내 자신(차량)이 비틀거리며 운행한다면? - 사고가 날 수 있음.
* 어떤 차선에 있는지 정확하게 알아야 경로를 분류할 수 있게 됨.
* 정확한 위치정보를 힌트삼아서 지도를 잘 만드는 기술 - 맵핑
* 정확한 지도를 힌트삼아서 위치를 잘 추정하는 기술 - 로컬라이제이션

* 주어진 힌트로 인해서 제약조건이 생김. - 힌트가 없을때!
* Monte Carlo localization - 지도가 사전정보가 주어졌을 때, 파티클 필터를 사용하여 위치를 추종함
* 파티클 필터 - 모션정보와 옵져베이션(관찰) 정보 (Pro, Ext 센서의 확률분포를 조합해서 최적의 정보(Pose, 위치정보)를 갖는 것)
* 1. initialization 단계 - configration space에 파티클을 쫙 뿌림. (파티클은 로봇이 위치 할 수 있는 정보)
* 2. motion update 단계 - 뿌려진 파티클마다, Pro 센서로부터 들어온 모션 정보를 추가하여 위치 정보를 업데이트 - 파티클이 벽에 들어가거나 존재할 수 없는 위치는 삭제
* 3. measurement 단계 - 뿌려진 파티클 마다, Ext 센서로부터 들어온 옵져베이션 정보를 덧씌움.
* 4. weight update 단계 - 해당 위치에 실제 옵져베이션 값이 나올 수 있는지 파티클의 현재 위치 정보와 주변환경 정보가 맞는지 계산을 수행
* 5. resampling 단계 - 현재 위치와 주변 환경정보가 맞아 떨어지는 파티클만 남기고 주변에서 파티클을 새로 뿌려서 재작업.
* 지도속에서 어디에 로봇이 존재해야 위치와 환경정보가 맞아떨어지는지 찾는 과정을 누적하면서 정답에 가까워지는 것이 몬테칼로 로컬라이제이션의 핵심.
* 비슷한 공간이 있을 때, 위치를 헷갈릴 수 있다 - 이는 몬테칼로 알고리즘의 특징. 또한, 몬테칼로는 지도를 전적으로 믿는다는 점을 알 수 있다.

* 닭이 먼저인가? 알이 먼저일까? - 로컬라이제이션이 먼저되어야 하는가? 맵핑이 먼저 되어야하는가? 문제
* 예전에는 좋은 센서를 통해서 해결 하였음. - 가격적인 문제가 있음. 그리고 제약조건이 있었음 (실내, 실외)
* 완전히 새로운 환경에서는 어떻게 풀어야 했을까? - 원래는 문제를 풀 수 없었음. 이를 해결하기 위해 SLAM이 나옴.
* SLAM은 아무 사전 정보 없이 Pro, Ext 정보를 동시에 받아 동시에 추정하는 것

* SLAM은 위치 정보나 지도 정보가 아예 없거나 불완전할때 사용가능
* 최적의 위치, 지도 정보를 추론 가능하고, 아무런 사전정보가 없어도 됨.
* Localization은 높은 수준의 지도가 있을 때, 위치를 추론하는 방법
* Mapping은 높은 수준의 위치정보가 있을 때, 지도를 추론하는 방법
* 가장 중요한 것은 높은수준의 지도와 위치정보가 있으면 만사형통. - SLAM도 안써도 됨.

### SLAM의 종류
* SLAM에서 사용 할 수 있는 센서
* 지난 시간에 Pro, Ext 센서를 배웠음.

* Proprioceptive sensor의 종류 - IMU, Wheel encoder

* Wheel encoder - 바퀴의 회전량(RPM), 이동량(=바퀴의 회전량 * 바퀴의 둘레)을 측정하는 센서
* 브러시 인코더나, 빛을 이용한 옵티컬 센서, 자기장, 전기를 이용한 마그네틱, 인덕티브 캐패시브 센서를 사용
* 이를 이용한 알고리즘 - 데드레커닝 기법. - 모션값을 누적해가면서 추정함. 
* 장점 - 자동차에는 기본적으로 탑재 - 로봇, 드론에도 포함되어 있음.
* 단점 - 시간이 오래지날수록 에러가 누적되는 단점, 바퀴가 헛도는 경우 잘못된 센서의 값이 생길 수 있음(비가 오거나 눈이 오는 경우).
* 단점 - 바퀴의 둘레가 주행중 너무 자주 바뀜 (탑승자의 무게, 코너링, 바람빠짐, 마찰열로 인한 타이어팽창)

* IMU - 선형가속도(Linear accelerator)를 측정하는 센서와 각속도(Angular gyroscope)를 측정하는 센서가 혼합된 센서
* Spring-damper system의 원리를 이용함
* Optical system - 차량용 IMU
* MEMS - 스마트폰 및 소형 디바이스 IMU
* 장점 - Consumer grade 제품은 저렴한 편 (자동차는 동일 성능에 저렴하지 않음), 높은 sensitivity, 높은 fps(100hz ~ 4000hz)
* 단점 - 엄청나게 빠른 drift 누적 - 보정을 위해 Camera, LiDAR, GNSS와 함께 사용, bias 값이 점점 바뀌는 모습을 보임 (기준 값과 노이즈가 함께 날라감)
* 비전과 융합한 VIO,VINS 라이다와 결합한 LIO, LINS가 유행하고 있음 - IMU를 통해 얻은 프라이오 정보를 다른 센서와 결합해서 좀 더 빠르게 얻기 위함.
* 

















