# 강의 요약

## 3D회전과 이동

### Rigid Body
* 위치에 대한 정보를 x, y, z 값으로 표현 가능
* 단순히 위치 값이 아닌, 방향도 있음.
* ex) 카메라 - 카메라가 어떤 방향을 보는지도 알 수 있음.
* Position (위치) + Orientation (방향) = Pose
* 카메라좌표계의 tx, ty, tz 를 월드 좌표계에서 본다면? - 좌표 변환이 필요함.
* 좌표 변환에 필요한 점 - 2개 좌표에 대한 Orientation 좌표와 Position 좌표가 필요함.
* Orientation - 회전값, Position - 평행이동값
* Ridge body motion - 3D -> 3D transform (유클리디안 변환, 유클리디안 월드 - 또다른 유클리디안 월드)
* 유클리드 기하학에 맞춘 법칙 - 유클리드가 붙음.
* 유클리드 공간에서 좌표계 표기법 - Cartesian Coordinate, 우리가 살고있는 공간은 유클리드 공간.

## 다양한 회전 표현법
### Euler Angle
* Roll / Pitch / Yaw
* 각각의 축을 의미, 3D rotation을 표현하기 위해 차례로 변환을 해줘야함.
* 장점 - 이해하기 쉬움
* 단점 - 최적화 하기 어려움 (미분이 가능해야 최적화 가능), 짐벌락 (Gimbal lock)
* 짐벌락 : 특정한 축의 위치에서 한 좌표가 회전할때, 3D 좌표가 2D 좌표처럼 표현되는 것을 의미 (하나의 회전 각도가 사라짐)

![image](https://user-images.githubusercontent.com/55529455/170526592-c35e6301-3689-488f-b28c-779df9bbc013.png)

### Axis-angle
* 물리학에서 벡터를 바라보는 방향으로 보는 것.
* 방향과 크기를 가지는 것이 벡터임.
* 회전에 하는 축에 방향과 얼마나 회전을 하는지 각도 2개를 따져서 물리학에서의 시점으로 봄.
* 축 + 각 = 축각도 = 로테이션 벡터 = 로드리게스 공식
* 메모리의 효율이 다른 회전 표현법보다 좋음. (3개의 파라메터로 각도 표현 가능함)
* 4개의 파라메터는 9개보다 2배 이상 좋고, 3개만 쓰면 메모리 효율이 3배 좋아짐.

![image](https://user-images.githubusercontent.com/55529455/170558030-ea171b27-c90f-4f66-9319-ea497173f9ee.png)

### Quaternion
* 회전을 표현하기 위한 알맞은 회전축의 개수? - 3개면 안되나? : 이 방식은 싱귤래리티 문제 발생
* 위 문제를 발생하지 않는 최소한의 파라메터는 4차원으로 표현하면 됨.
* 쿼너티엄은 4개의 파라메터를 사용하는 것.
* No singuarity로 인해 미분이 가능해짐 - 최적화가 가능해짐. SLAM에 사용하기 좋음. - 단, 4차원에 대한 이해가 필요함.

### SO(3) Rotation Matrix
* Special Orthogonal Matrix (3D) - SO(3)
* 3개의 파라메터로 구성됨. 회전만 표현해주는 매트릭스.
* 1. x,y,z가 회전을 해도 기존의 x,y,z축이 모두 회전 후에도 수직을 유지해야함.
* 2. x,y,z축이 벡터의 길이가 바뀌면 안됨.
* 즉, 행렬식이 1이 나와야 한다는 점이다. ((a b) (c d))가 있다면, ad - bc
* 역함수(A^-1)는 매트릭스(A)에 트랜스포즈(A^T)를 한 값이랑 같다.
* Rx 순으로 Rz까지 곱한다.
* 여러개의 회전을 중첩해서 하나의 회전을 표현하는데 좋은 방법
* 하지만, 3축의 회전을 표현하기 위해서 9개의 파라메터를 사용해야함. - 메모리를 많이 차지함. 
* 여러개의 매트릭스를 1개의 매트릭스로 통합 할 수 있다는 점이 있다. - 메모리를 어느정도 절약 가능함.
* 조건들이 상당히 있음. - 최적화를 하기 위해서 바로 사용하는 것은 어렵다.
* 미분은 가능함. - 자코비안 매트릭스를 얻을 수 있지만, 반복하게 된다면, 반복 결과가 SO(3) 결과가 나올지는 모름. (반복 = 이터레이션)

![image](https://user-images.githubusercontent.com/55529455/170562244-6a4cbeba-701f-42af-a90f-9b1d4e422a1c.png)

### Translation
* {x, y, z}로 표현함
* 벡터로 표현해주면 됨. - mm, cm, m 단위만 잘 조절해주면 됨.

### Transformation
* SE(3) 매트릭스 - SO(3) 매트릭스 + Translation (4x4)
* Special Euclidean Group (3D) \[ (R(3x3), t(3x1)), (0, 1) ]

![image](https://user-images.githubusercontent.com/55529455/170563975-42742139-a39b-4e61-aa36-8c070c80fcf7.png)

## Projective geometry - 사영기하학
### 3D World vs Photo
* 닮음을 이용한 Similarity Geometry, 한점만 이동 시키는 경우에 가능한 Affine Geometry가 있음.
* 사영기하학도 하나의 기하학임.
* 사진 속 차선을 생각했을 때, 유클리디안 기하학에서 평행함 (무한의 거리에서 두 선이 교차함 - 유클리디안에서는 무한은 숫자 표현 안됨.)
* 유클리디안 에서 표현하던 무한의 거리를 픽셀에서는 표현할 수 있는 영역이 점점 작아지면서 교차하게 된다.
* 미술학에서는 소실점이라고 함.

### 원근법
* 실제 세계에서는 평행이었던 것이 사진에서는 평행하지 않는가? - 원근법 (가까이 있는건 크게 보이고, 멀리 있는건 작게 보임)
* 우리는 물체의 실제 크기를 알 수 없음. 3D의 데이터를 2D 이미지로 투영하면서 depth 이미지가 소실됨.
* 유클리디안 트랜스폼이 되기 위해서는 orthogonality 유지, 평행 유지, 여러가지가 있음.
* 프로젝티브 트랜스폼은 직선인 물체가 직선으로만 나오면 된다.

### Vanishing point (소실점)
* 유클리디안에서는 평행한 직선은 무한에서 교차하는데 무한은 관측 불가능함.
* 무한의 거리를 소실점에서 관측 할 수 있게 됨.
* 3D에서의 무한은 2D에서는 유한으로 매핑이 가능해지는 것이다. - 수학적으로 표현이 가능해진다.
* 3D에서 2D를 매핑할 때 필요한 모든 위치를 수로 표현할 수 있고, 무한의 교차점을 표현 할 수 있는 것 - 2가지가 새로운 기하학의 특징이 됨 (사영기하학)

### Hierarchies of geometries
* 유클리디안, 시밀러티, 어파인, 프로젝티브
* Euclidean - Rotation + Translation 표현 가능
* Similarity - Euclidean + uniform scaling 표현가능, length 정보 소실
* Affine - Similarity + Non-uniform scaling + shear 표현 가능, Angle / length-ratio 정보 소실
* Projective - Affine + projection 표현 가능, incidence , cross-ratio 정보 소실
* Projective geometry - N+1 차원 > N 차원 투영 (차원을 낮출 때 사용하는 기하학)
* {x, y, z} + {scale} = Homogeneous coordinatiates

## Homogeneous coordinates
### Definition (Plucker definition)
* Homogeneous coordinates 정의
* x라는 개체에 Homogeneous coordinates 0이 아닌 어떤 스칼라 값을 곱해도 coordinate는 같은 값을 의미함.
* x = lambda * x 라고 표현 할 수 있음.

![image](https://user-images.githubusercontent.com/55529455/170579728-b5a71336-b8c9-4632-87e1-3c4aa2aca5d4.png)
![image](https://user-images.githubusercontent.com/55529455/170579880-2e1611a7-2b79-4dcf-9fcb-a182377b21ad.png)

* 기존의 차원과는 다른 scale의 차원이 생김.
* x = {x, y} 일 때, x = {x, y, 1}로 표현이 됨. - scale의 값이 1임
* 어떠한 스칼라가 곱해져도 결국은 scale 값으로 나눠주면, {x, y, 1}로 돌아옴.
* {x, y} = Cartesian coordinates, {x, y, 1} = Homogeneous coordinates

### Projective space
* u, v 는 x, y축 (2D Plane의 좌표계)
* w 축? - 호모지니어스 좌표계의 scale을 의미함.
* 색칠되어있는 plane = 유클리디안 공간 - Projective 속에 있는 하위 공간을 의미함.
* x 점 좌표는 어떻게 표현이 될까? (0,0,0) ~ (x, y, 1)을 지나서 lambda(x, y, lambda)로 표현이 될 수 있다.

### Euclidean space vs Projective space
* 유클리디안 공간 - Cartesian coordinates, (사영공간의 일부, scale 값이 1), 호모지니어스로 변환시 scale 값을 1을 추가하면 됨. (x,y,1)
* 사영 공간 - Homogeneous coordinates, 카티션으로 변경시, scale을 1로 만들고 scale 부분 벡터를 떼면 됨.
* 매트릭스를 호모지니어스로 변환이 가능한가? - 가능함. 3x3의 매트릭스를 4x4로 만들고, 마지막 부분에 0, 0, 0, scale 을 추가해주면 된다.

![image](https://user-images.githubusercontent.com/55529455/170582740-3c590b53-0865-4a2d-aa31-ec56a555659c.png)

## 핀홀카메라 투영
### Camera obscura
* 바늘구멍 사진기 (pinhole camera) - 정확한 2D 이미지를 구할 수 있었음
* 3D 세상과 비교하면서, mapping 수식을 발전 할 수 있었음.
* 두가지 물리법칙
* 1. 빛은 직선으로 이동한다. 이미지에 맺힌 상이 거꾸로 맺힌다는 것도 직선으로 빛이 이동한다는 것
* 2. 상맺힘 - 3D 이미지가 2D 이미지로 하나의 상으로서 투영됨.

### Morden camera - 핀홀 카메라와 원리는 같다.
* 렌즈를 통해 빛을 모으고 초점을 맞추기 위해 sw/hw 수정을 함.
* 하드웨어 수정 - 구멍의 크기를 바꾸거나, Exposure를 통해 전체 광량을 수정
* 소프트웨어 수정 - ISO gain, normalization을 통해서..

### Camera Projection
* 3D -> 2D 투영을 수학적으로 설명
* x = PX (X는 3D 코디네이터 포인트, P는 프로젝션 매트릭스, x는 2D 코디네이터 포인트)
* world coordinate system (3D) -> camera coordinate system (3D) -> image coordinate system (2D)
* 월드가 바라보는 위치를 카메라로 바라보겠다는 것 + 차원 축소

### world coordinate system (3D) -> camera coordinate system (3D)
![image](https://user-images.githubusercontent.com/55529455/170584065-6560e67a-4d03-4906-a366-368172c79d34.png)
![image](https://user-images.githubusercontent.com/55529455/170589909-6df2cf15-1438-46d7-b5e4-cf6c9cb9c31d.png)
* world frame은 사전에 우리가 정한 좌표를 의미함.
* camera frame은 카메라의 이미지 센서의 중심에 있음.
* world frame - camera frame 이동하면서, Xw가 Xc로 변환이 됨.
* 물체위치를 표시하는 방식을 물체와 카메라와의 상대적인 Pose로 하겠다는 것을 의미함. - world와 관련된 것을 모두 제거
* Xc가 가상의 이미지를 투과하는 것을 볼 수 있다.

![image](https://user-images.githubusercontent.com/55529455/170590365-bfe50d82-928b-4f46-b57f-1ce02122c0b1.png)
* z축은 카메라의 전방을 의미
* Image plane의 영점을 P라고 함 - 주점이라고 함.
* x_c를 보면, 가상의 image plane을 통과하는 것을 볼 수 있음 - x_im
* 이상한점? - 상이 똑바로 맺혀있다? 반대로 뒤집혀 있다고 배웠는데..
* focal point를 지나서 뒤집혀있는 것이 맞음. 하지만, focal point 앞으로 투영된 이미지를 사용하는 것이 관용
* focal point 앞으로 나둔 이미지는 뒤에 뒤집힌 상을 180도 회전 시킨 것과 같은 결과다.
* 수식에서 결과가 다르게 나오지 않음.

![image](https://user-images.githubusercontent.com/55529455/170590766-410b3d91-8fee-4d25-829e-39d62de7ff48.png)
* fx, fy = focal length (focal point와 가상의 image plane 사이의 거리)
* (x, y, z)를 (fx\*x/z, fy\*y/z)로 표현 할 수 있게 되었다. 하지만, C의 기준으로 되어있고, 이미지의 중앙으로 되어있음.
* 픽셀로 이미지를 표현할 때는 0,0은 이미지 좌측상단을 의미한다.

![image](https://user-images.githubusercontent.com/55529455/170591134-53f43704-199f-41d3-b5fe-f34efb97dfc0.png)
* 따라서 이미지의 중앙을 왼쪽 상단으로 옮겨주어야한다.
* width / 2, height / 2 의 값을 가져온다. 하지만, 실제 값으로는 어느정도 에러가 있다.

![image](https://user-images.githubusercontent.com/55529455/170591230-1e4e4649-0367-4421-bdda-da905cd33379.png)
![image](https://user-images.githubusercontent.com/55529455/170591388-61b21a10-908b-4fe0-b62a-c346b9f806ca.png)
![image](https://user-images.githubusercontent.com/55529455/170591429-9eb0476e-1654-4251-b187-4e398b58cd84.png)

## 카메라 센서의 구조
### Camera
* 카메라는 사진을 찍을 수 있고, 기록을 남기는 것을 의미한다.
* 하지만, 컴퓨터 비전에서는 정확하게 빛의 정보를 담을 수 있는 것이 좋은 카메라
* 카메라는 단순히 빛을 감지하는 센서로 인지하게 될 것이다.
* 정확하게 빛을 감지하는 것은 어떻게 알 수 있는가?

### Steps to capture light
* 모든 카메라 센서는 포토 다이오드 센서로 빛을 받아들여 전압을 생성하고, ADC로 디지털 신호로 변환함.
* 컬러 카메라에는 베이어 패턴이라는 RGB 형태로 이미지를 얻어냄.
* 10bit, 12bit 16bit로 데이터를 얻게 됨. 8bit 보다 훨신 섬세하게 빛의 밝기를 표현
* RAW - 이미지 센서가 그대로 받아들인 데이터 - 양자화를 통해서 SRGB colorspace 로 매핑하면 컬러 이미지로 됨.
* 깊게 들어간다면?
* 피사체에 충분히 빛을 반사시켜 받아들이고, 광량을 조절한다. CMOS 센서로 이미지 프로세싱을 함.
* SRGB colorspace를 받아서 보정한 후, 실제보다 더 이쁜 이미지가 만들어짐.
* CMOS의 내부 이미지 프로세싱에 집중.
* 렌즈를 통해서 빛이 들어오고, CMOS센서가 빛을 받아 디지털 신호로 바꿈.
* ISO gain 값을 통해 밝기를 보정해주고, 베이어 패턴에 맞아서 디모자이킹을 통해서 RGB 색상을 만들어줌.
* 이후, 노이즈를 제거한다. 화이트 밸런싱 작업을 하고, 밝은건 더 밝게, 어두운건 더 어둡게 한다.
* sRGB 매핑을 해주고, 이미지를 저장하기 위해서 압축작업을 하여 저장한다.

## 카메라의 종류
### Many different types
* RGB, Monochrome, Polarized, Infra-red, Multi-, Hyperspectral, Event, Light-filed
* 이전 영상처리를 하는데 있어서 그레이스케일 기반 영상을 처리하였다.
* 흑백 영상을 쓸텐데 왜 흑백 카메라를 쓰면 안되나? - 흑백 카메라가 훨신 좋다.
* 하지만, 보통의 카메라는 컬러 카메라를 사용하기 때문이다.

### RGB Camera
* 베이어 패턴 : 픽셀마다 RGB 중 하나의 필터를 씌우고, 두개의 같은 컬러의 픽셀 사이에 있는 픽셀을 인터폴레이션(보간)으로 풀어서 모든 픽셀의 값에 RGB값이 나타나도록 하는 것.
* 단점 - 실제 센서 데이터가 아니라 주변 픽셀의 값을 이용한 추론이므로 정확한 값을 받는 것이 아님.
* 즉, 베이어 패턴을 나타내는데 있어서 오차가 매우 많이 남. (블러링과 같은효과로 흐려짐)
* 픽셀들마다 2번씩 인터폴레이션(보간법)계산이 들어감. 
* 단점 - 베이어 패턴에 의해서 RGB 중, 하나의 필터가 픽셀에 씌어지게 되는데, 이 필터로 인해 모든 빛의 스펙트럼을 받을 수 있게 설계되었음에도 하나의 색을 제외한 나머지 빛색깔을 받지 못함.
* 그렇기 때문에, 절대적인 광량이 줄어들고, 영상의 밝기가 어두워짐.

### Monochrome camera
* RGB 컬러 패턴을 위한 베이어 패턴이 없다는 점이 있음.
* 이로 인해 이미지 퀄리티가 매우 좋아짐.
* 빛의 스펙트럼을 전부 받을 수 있기 때문에 광량이 많아짐
* 광량이 많아진다? - 노이즈 값에 비해 빛의 신호양이 많아지므로 신호 대비 노이즈 양이 적어짐
* 이 덕분에 훨신 깔끔한 이미지가 나오게 됨.
* 컬러 이미지는 색깔이 번지는 반면, 흑백 이미지는 구분이 쉽게 됨.
* 컬러 이미지의 어두운 면을 ISO gain을 하면서 올릴 수 있긴 한데, 노이즈가 심해짐.

### Multi/Hyperspectral camera
* 기존의 컬러 카메라의 RGB의 값을 받았다면, 더욱 많은 컬러를 받을 수 있다.
* 사람이 볼 수 없는 영역의 빛까지 받을 수 있음.
* 10개 채널정도 되면 Multi, 100~200개 채녈이 되면 Hyperspectral
* V-SLAM에서 사용되는 카메라는 아니지만, 적외선 카메라에 사용함.
* 야간 카메라, 열감지, 투시 카메라 - 크래프션, 야간 V-SLAM, 화재 V-SLAM에서 사용하는 경우가 있음.

### Polarized camera (편광 카메라)
* 편광 필터를 얹어서 사용하는 카메라
* 금속, 비닐로 인한 반사된 빛으로 인해서 시야가 가려져 V-SLAM이 종료되는 경우가 있음.
* 편광판이 있으면, 해당 노이즈를 걸러주어 정확하게 보여줌.
* 하드웨어에서 편광판을 써야 해당 문제를 해결 할 수 있다고 알려짐. - SW는 불가능

### Event camera
* 픽셀의 밝기 변화가 있을 때, (이벤트가 있을 때) 신호를 감지함.
* 주기적으로 이미지가 들어오는 카메라와 다르게, 이벤트가 생길때만 데이터를 수집함.
* 기존의 카메라보다 빠르게 작동 가능 - 초고속 카메라와 맞먹는 수준
* V-SLAM에서 떠오르는 강자
* 빠른 모션에도 안정적으로 검출하는 것이 가능함.
* 이미지의 밝기가 낮아도 잘 검출함.

## 좋은 카메라 고르는 방법
* Image sensor - 카메라 안에 있고, 특정 빛을 모아서 이미지를 그림.
* Lens - 옵스큐라(암상) 할 수 있도록 빛을 모아서 바늘 구멍을 통과시켜주는 장치
* 이 두가지가 중요함.
* 카메라 안좋고, 렌즈가 좋으면? 잘 모아도 잘 그리니 흐린 이미지가 나옴.
* 카메라가 좋고 렌즈가 안좋으면? 잘 그려도 잘 못모으니 흐린 이미지가 나옴.
---
* 센서 해상도(sensor resolution) - 물체의 정보를 세부적으로 담을 수 있음. 좀 더 작은 객체까지 표현 가능
* 단, 연산시간이 늘어나게 됨.
* 센서 크기가 고정되어있고, 그에 비한 많은 픽셀을 담으려면 우겨넣어야하므로 픽셀 면적이 줄어들고, 빛을 덜 받음 (노이즈, 어두워짐)
---
* 센서 민감도(sensor sensitivity) - 적은 양의 빛으로 밝은 이미지 얻을 수 있음.
* 단, 금방 이미지가 하얀색으로 될 수 있기 때문에, 인위적으로 필터를 씌우게 됨.
---
* 초당 프레임 수 (FPS) - 시간에 대한 해상도가 높다는 것.
* 실시간 비전 시스템은 25 ~ 30 정도는 되어야함.
---
* 센서 크기 (sensor size) - 센서 사이즈가 크면, 픽셀 수도 자연스럽게 커짐.
* 이로 인해 깔끔한 이미지 얻을 수 있다. 외각으로 갈 수록 생기는 왜곡 효과도 피할 수 있다.
---
* Noise factors - shot noise, Read noise가 있음.
* 둘 다 낮은게 좋음.
* Quantum efficiency - 실제로 받은 광저중에서 얼마나 디지털 신호로 변경되는지? 변경 안된 신호는 노이즈. 높을수록 좋다.
* Heat stability - 더운 환경, 추운 환경에서의 노이즈 원인
---
* Dynamic range - 밝은곳과 더 밝은 곳이나 어두운 곳, 더 어두운곳을 구분 할 수 있는 것
* 다이나믹 레인지가 낮으면 그냥 비슷하게 보일것이다.
---
* CCD - 대부분 카메라 채택, DSLR에 들어가있는 센서, 글로벌 셔터 가능, 높은 다이나믹 레인지, 적은 노이즈지만 크기가 큼.
* CMOS - 초소형 카메라 채택, 저전력, 롤링 셔터 이펙트가 있음. 글로벌 셔터가 가능한 CMOS를 선택해야함. 큰 공간에서 안정적 결과 도출
---
* Global shutter - 스냅샷을 찍을 수 있음.
* Rolling shuuter - 움직이는 카메라에서 발생되는 현상, 위에서 아래로 사진을 그리는데, 픽셀이 밀림 (해결법 : 스냅샷 가능)
---
* 카메라의 크기 - 쿨링, 파워, 인터페이스 등으로 인해 카메라의 크기가 결정됨. 무게에 민감한 드론이나, AR 글래스는 중요해짐.
---
* 카메라마다 사용처에 따른 쿨링과 파워가 다름.
* USB 카메라는 USB 정도로 해결 가능함.
* Power Interface와 cable이 많아질수록, 무게가 늘어나게 됨.
* 카메라의 성능이 뛰어날수록 빠르게 이미지 처리를 하거나 이미지 버퍼를 가지고 있거나, 빠른 플러쉬레이트가 있으면 소형 컴퓨터가 들어있다고 봐야함.
* 쿨링팬을 달아주지 않으면 멈춤.
* 카메라의 성능은 좋지만, 전체적인 무게가 늘어나게 됨.
---
* 카메라의 인터페이스도 중요함. USB, USB-C, Gig-E, 등
* 케이블의 길이도 일정 수준으로 권장되는 길이가 있다. (넘어가면 속도 저하)
* 손에 들고다니거나, 하드웨어에 부착하거나, 짐벌이나 삼각대, 벽 등에 설치하는 경우도 있음.
---
* 빠른 계산을 위해서 전처리 센서와 ISP 프로세서가 있는 경우가 있다.
* 굉장히 빠른 이미지를 처리할 수 있지만, 쿨링 케이스 필요 (발열이 있음)
* 펌웨어에 접근해서 역패키징을 통해서 원하는 계산만 커스터마이징도 가능함.
---
* 온도변화에 강한 것도 중요하다. (땡볕, 추운 곳)
* 방진과 충격도 중요해짐.
---
* 렌즈 - 5개의 포인트를 중점으로 봐야한다.
* Sensor size
* Working Distance
* Resolution
* Depth of Field
* Field of View

![image](https://user-images.githubusercontent.com/55529455/170690509-26126de0-0d56-4a32-b376-5b7be4bc3b14.png)

* 위 5개 이후에 더 정한다면,
* Spatial resolution (공간 분해능)
* Distortion
* Perspective (시야각)
* Contrast
---
* 공간 분해능 vs 센서 해상도
* 렌즈에 대해서는 레졸루션이 분해능이라고 하고, 센서 부분에서는 해상도라고 함.
* 센서 해상도는 센서가 표현 할 수 있는 최대 정확도 - 해당 점과 주변을 어떻게 표현할 수 있는가
* 공간 분해능은 공간을 얼마나 쪼개서 보낼 수 있는지에 대한 정확도 - 한 점을 얼마나 정확하게 표현하는가

### Focal
* 렌즈는 비싼거 써도, 모든 카메라에서 좋은 결과를 얻을 수 없음.
* 센서 사이즈로 인해 결정되는 focal length, 피사체의 거리와 렌즈 화각의 거리에 따라서 형태가 변함
* 센서 사이즈, 피사체 거리, 시야각의 조화가 잘 맞는 렌즈가 좋은 초점을 얻을 수 있는 렌즈임.
* lens cal을 통해서 계산은 가능하지만, 초점은 맞지만 정확한지는 모름.

### Field of View
* 2가지 용도의 렌즈
* 이미지가 깔끔하게 나온다는 정의가 다름.
* 광각렌즈, 망원렌즈로 구성됨.
* 광각렌즈 - 넓은 화각은 가지지만, FOV 끝에 있는 빛과 중앙의 빛과 차이로 인해 퀄리티가 줄어듬. 점점 둥글어져 디스토션이 생김.
* 망원렌즈 - FOV가 비슷한 이미지 퀄리티를 가지지만 초점거리가 많이 멀기 때문에, focus를 맞추기 쉽지 않음.
* 깔끔한 이미지를 주는 렌즈는 어떤 렌즈인가? - MTF Chart를 보고 판단
* MTF Chart - 데이터 시트로 제공됨. x축은 특정 공간 반복되는 패턴 수(반복될수록 세세한 패턴), y는 컨트러스트 (높을수록 잘 검출)
* x 값이 커지면, y 값이 내려가게 되는데, 천천히 내려가는 렌즈가 좋은 렌즈

