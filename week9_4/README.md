# 강의 요약

> R-TOD 논문

> 인공신경망
> 기계학습 역사에서 가장 오래된 기계 학습 모델
> 1950년 퍼셉트론 (인공두뇌학)
> 1980년 다층 퍼셉트론 (결합설)
> 2000년대 깊은 인공신경망 (심층학습)
> 현재 다양한 형태의 인공신경망을 가지며, 주목할 만한 결과를 제공함
> 3장은 깊은 인공신경망 (심층학습 4장)의 기초가 됨.

> 사람의 뉴런
> 두뇌의 가장 작은 정보단위
```
구조
세포체는 간단한 연산 | 노드
수상돌기는 신호 수신 | 입력
축삭은 처리결과를 전송 | 출력
시냅스 | 가중치
```
> 두 줄기 연구의 동반상승 효과 (시너지)
> 컴퓨터 과학 = 컴퓨터의 계산 (연산) 능력의 획기적 발전
> 뇌 과학 (의학) = 뇌의 정보처리 방식 규명 연구
> 컴퓨터가 사람 뇌의 정보처리를 모방하여 지능적 행위를 할 수 있는 인공지능 도전
> 뉴런의 동작 이해를 모방한 초기 인공신경망 연구 시작 -> 퍼셉트론 고안

> 인공신경망은 다양한 모델이 존재함
> 전방 신경망과 순환 신경망
> 얕은 신경망과 깊은 신경망

> 결정론 신경망과 확률론적 신경망 비교
> 결정론 신경망 - 모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 신경망
> 확률론적 신경망 - 고유의 임의성을 가지고 매개변수와 조건이 같더라도 다른 출력의 가지는 신경망

> 퍼셉트론
> 구조 - 절(노드), 가중치(웨이트), 층(레이어)과 같은 새로운 개념의 구조 도입
> 제시된 퍼셉트론 구조의 학습 알고리즘을 제안
> 원시적 신경망이지만, 깊은 인공신경망을 포함한 현대 인공신경망의 토대 - 깊은 인공신경망은 퍼셉트론의 병렬 배치를 순차적으로 구조로 결합됨
> 현대 인공신경망의 중요한 구성 요소가 됨.

![image](https://user-images.githubusercontent.com/55529455/163405998-83490c48-fa8f-487e-b4e4-6ba1ce3b09f7.png)
![image](https://user-images.githubusercontent.com/55529455/163406045-7d5ea8bf-3e64-4fa2-ad6d-f132066a26a2.png)
![image](https://user-images.githubusercontent.com/55529455/163406086-a3b1bfc6-19b5-4f7a-bcee-3902abd37494.png)
![image](https://user-images.githubusercontent.com/55529455/163406106-32067ecf-0d83-48fd-87ff-43265424f9f8.png)
![image](https://user-images.githubusercontent.com/55529455/163406161-467e3393-b243-427f-a9ed-0258a8dc6dd6.png)
![image](https://user-images.githubusercontent.com/55529455/163406198-f1b50701-3534-4dfd-8acb-b835e658dc05.png)
![image](https://user-images.githubusercontent.com/55529455/163406230-967c3792-336b-4f5d-b3f0-73064fa2b7c4.png)
![image](https://user-images.githubusercontent.com/55529455/163406267-a8061f9f-f48a-4561-9662-62bfc2683f33.png)
![image](https://user-images.githubusercontent.com/55529455/163406316-e178e582-a22d-45de-ae60-bb6faac3f808.png)
![image](https://user-images.githubusercontent.com/55529455/163406345-595318bc-ecb7-48a2-b4e2-2d04edaca3aa.png)
![image](https://user-images.githubusercontent.com/55529455/163406465-9ff65050-f91c-4a63-87c1-7e34dc7345c8.png)
![image](https://user-images.githubusercontent.com/55529455/163406487-7266d7d4-a2d1-455c-a253-6196e7e9bc27.png)
![image](https://user-images.githubusercontent.com/55529455/163429730-56e531dd-269d-41ed-9784-afdc32fd3bb0.png)
![image](https://user-images.githubusercontent.com/55529455/163429761-6c845295-260d-4136-b640-75c762c5bf23.png)
![image](https://user-images.githubusercontent.com/55529455/163429787-52d8952d-9a76-41bd-8d6b-3b6b9a748d27.png)
![image](https://user-images.githubusercontent.com/55529455/163429815-4382032c-c95d-46c9-b323-ae638ad69c6f.png)
![image](https://user-images.githubusercontent.com/55529455/163429842-965f362c-f96e-4fc1-b3ca-b52412a753e4.png)
![image](https://user-images.githubusercontent.com/55529455/163429902-36c29ef6-68ab-4fe0-bfc1-65c55c91cf46.png)
![image](https://user-images.githubusercontent.com/55529455/163429928-9f181970-6e73-4263-ba72-3100b923f357.png)
![image](https://user-images.githubusercontent.com/55529455/163429968-d6d6f2a6-b942-4c31-9418-1870cce715d6.png)
![image](https://user-images.githubusercontent.com/55529455/163430006-3afee4f6-ba1e-419f-8c56-eed2efd345a5.png)
![image](https://user-images.githubusercontent.com/55529455/163430051-d9fe2a01-c6c5-44e6-8ea6-06189d766152.png)
![image](https://user-images.githubusercontent.com/55529455/163430079-7b50594b-eec8-4d80-9029-e265aab11a1e.png)
![image](https://user-images.githubusercontent.com/55529455/163430109-87d088f1-b170-41b6-8c53-f065db5e9ed3.png)
![image](https://user-images.githubusercontent.com/55529455/163430143-0b2d3365-001a-4e1f-a8b0-fe4a6ee3c87e.png)
![image](https://user-images.githubusercontent.com/55529455/163430161-0363fb91-ea79-4051-a9a1-ceffdbbe8bcb.png)
![image](https://user-images.githubusercontent.com/55529455/163430185-8e796f2f-c70f-4062-b8ea-9c8483e26c68.png)

> 은닉층 깊이에 따른 이점
> 지수의 표현
> 각 은닉층은 입력공간을 어디서 접을지 결정 -> 지수적으로 많은 선형적인 영역 조각들 -> 성능향상

> 다층 퍼셉트론 학습과정
> 입력층에서 은닉층을 통해 출력층으로 순방향 전파를 통해 오차 계산을 하고, 출력층에서 은닉층을 거쳐 입력층에 역방향 전파를 함.

> 예측단계
> 학습을 마친 후, 현장 설치하여 사용함.

![image](https://user-images.githubusercontent.com/55529455/163430448-f21de972-3e82-41ce-90fc-68479e1d322d.png)

> 다층 퍼셉트론 시각화
> https://playground.tensorflow.org

> 연산 복잡도 비교
> 오류역전파 = 전방 계산 대비 약 1.5 ~ 2배의 시간 소요 - 비교적 빠름 (연쇄법칙)
> c = 분류수, d = 특징 차원, p = 은닉층 자원, n = 훈련집합 크기, q = 세대 수
> 학습알고리즘은 오류 역전파를 반복하여 점근적 시감복잡도는 O((cp+dp)nq)

> 은닉층을 하나만 가진 다층 퍼셉트론은 범용 근사자 (Hornik 주장)
> 은닉노드가 충분히 많으면, 포화함수(활성함수)로 무엇을 사용하든, 표준 다층 퍼셉트론은 어떤 함수라도 원하는 정확도 만큼 근사화 할 수 있다.

> 순수한 최적화 알고리즘으로는 높은 성능 불가능
> 데이터 희소성, 잡음, 미숙한 싱경망 구조 등 이유
> 성능 향상을 위한 다양한 경험(휴리스틱)을 개발하고 공유함

> 신경망의 경험적 개발에서 중요 쟁점
> 아키텍처
> 초깃값
> 학습률
> 활성함수

![image](https://user-images.githubusercontent.com/55529455/163517853-3ad994c3-d4a5-4773-a0bc-5e5129999a77.png)
![image](https://user-images.githubusercontent.com/55529455/163517869-30427421-05c1-4a63-815f-43205f94e7ac.png)
![image](https://user-images.githubusercontent.com/55529455/163517877-5abc8e90-2f09-40e4-94eb-1491535fcce2.png)

> 연산 그래프
> 연산을 하는 대상과 연산기호 자체를 통해서 어떠한 출력값을 보이는지 시퀀스를 나타내주는 것

> 연쇄 법칙의 구현
> 반복되는 부분식들을 저장하거나 재연산을 최소화 (DP)

![image](https://user-images.githubusercontent.com/55529455/163518054-5c08b8e4-5e5b-47be-8e9b-8f68450f363f.png)
![image](https://user-images.githubusercontent.com/55529455/163518087-73ccbcb7-5fef-4a0c-a385-7fed70684285.png)
![image](https://user-images.githubusercontent.com/55529455/163518107-5ceb8d6d-2108-4715-8896-d92da3f0d270.png)

> 오류 역전파 알고리즘
> 출력의 오류를 역방향(왼쪽)으로 전파하여 경사도(미분값)를 계산하는 알고리즘
> 현재의 미분값은 이전의 값에 영향을 주는 것을 이용함.
> 반복되는 부분식들의 경사도의 지수적 폭팔 혹은 사라짐을 피해야함.

![image](https://user-images.githubusercontent.com/55529455/163518440-0b215437-497e-42cf-95dd-5f3509b974c2.png)
![image](https://user-images.githubusercontent.com/55529455/163518443-0b59cdff-cb1b-484b-8ec0-a5673ae01baf.png)
![image](https://user-images.githubusercontent.com/55529455/163518460-f3c42991-a98e-4774-a92e-38771480b938.png)
![image](https://user-images.githubusercontent.com/55529455/163518477-437a46ea-fee9-4eae-8a40-67e6e832c39a.png)
![image](https://user-images.githubusercontent.com/55529455/163518488-6eb9ca94-adbc-4e81-bc40-d5e897e00279.png)
![image](https://user-images.githubusercontent.com/55529455/163518509-24b77a98-0490-452d-a893-a70dae12cfb6.png)
![image](https://user-images.githubusercontent.com/55529455/163518523-eb7bc0cf-055b-4b33-86cd-43c513a4ec26.png)
![image](https://user-images.githubusercontent.com/55529455/163518536-1029b825-cb23-4d4f-9e5d-b71407fcc193.png)
![image](https://user-images.githubusercontent.com/55529455/163518544-530be5a3-674e-4f8d-b50b-550064e8a7ce.png)
![image](https://user-images.githubusercontent.com/55529455/163518555-3918c927-0a1f-4611-a97c-799ad1678272.png)
![image](https://user-images.githubusercontent.com/55529455/163518562-9831510c-e61f-4c4c-b30c-e84bb5828e7c.png)
![image](https://user-images.githubusercontent.com/55529455/163518578-6fe11aa3-14ea-49df-b15d-e8799d664e87.png)
![image](https://user-images.githubusercontent.com/55529455/163518643-798db8b0-fe77-4de2-aed9-fd51ff6fdea9.png)

> 예전에는 전부 직접해야하지만, 지금은 알아서 해줌.

![image](https://user-images.githubusercontent.com/55529455/163519311-76b29bd1-fd02-4c39-a39a-2bbb38eef36c.png)
![image](https://user-images.githubusercontent.com/55529455/163519352-a64e83a8-ccb9-4125-ba4f-299aadbecf68.png)

> 미니배치 방식
> 한번에 t개의 샘픙르 처리 (t는 미니배치 크기)
> t=1이면 확률론적 경사 하강법
> t=n(전체)면, 배치 경사 하강법
> 미니배치 방식은 보통 t=수십~수백개
> 경사도의 잡음을 줄여주는 효과 때문에 수렴이 빨라짐
> GPU를 사용한 병렬처리에도 유리함
> 현대 기계학습은 미니배치 기반의 확률론적 경사 하강법을 표준처럼 여겨 널리 사용함




















