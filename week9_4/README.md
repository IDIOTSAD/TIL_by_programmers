# 강의 요약

> R-TOD 논문

> 인공신경망
> 기계학습 역사에서 가장 오래된 기계 학습 모델
> 1950년 퍼셉트론 (인공두뇌학)
> 1980년 다층 퍼셉트론 (결합설)
> 2000년대 깊은 인공신경망 (심층학습)
> 현재 다양한 형태의 인공신경망을 가지며, 주목할 만한 결과를 제공함
> 3장은 깊은 인공신경망 (심층학습 4장)의 기초가 됨.

> 사람의 뉴런
> 두뇌의 가장 작은 정보단위
```
구조
세포체는 간단한 연산 | 노드
수상돌기는 신호 수신 | 입력
축삭은 처리결과를 전송 | 출력
시냅스 | 가중치
```
> 두 줄기 연구의 동반상승 효과 (시너지)
> 컴퓨터 과학 = 컴퓨터의 계산 (연산) 능력의 획기적 발전
> 뇌 과학 (의학) = 뇌의 정보처리 방식 규명 연구
> 컴퓨터가 사람 뇌의 정보처리를 모방하여 지능적 행위를 할 수 있는 인공지능 도전
> 뉴런의 동작 이해를 모방한 초기 인공신경망 연구 시작 -> 퍼셉트론 고안

> 인공신경망은 다양한 모델이 존재함
> 전방 신경망과 순환 신경망
> 얕은 신경망과 깊은 신경망

> 결정론 신경망과 확률론적 신경망 비교
> 결정론 신경망 - 모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 신경망
> 확률론적 신경망 - 고유의 임의성을 가지고 매개변수와 조건이 같더라도 다른 출력의 가지는 신경망

> 퍼셉트론
> 구조 - 절(노드), 가중치(웨이트), 층(레이어)과 같은 새로운 개념의 구조 도입
> 제시된 퍼셉트론 구조의 학습 알고리즘을 제안
> 원시적 신경망이지만, 깊은 인공신경망을 포함한 현대 인공신경망의 토대 - 깊은 인공신경망은 퍼셉트론의 병렬 배치를 순차적으로 구조로 결합됨
> 현대 인공신경망의 중요한 구성 요소가 됨.

![image](https://user-images.githubusercontent.com/55529455/163405998-83490c48-fa8f-487e-b4e4-6ba1ce3b09f7.png)
![image](https://user-images.githubusercontent.com/55529455/163406045-7d5ea8bf-3e64-4fa2-ad6d-f132066a26a2.png)
![image](https://user-images.githubusercontent.com/55529455/163406086-a3b1bfc6-19b5-4f7a-bcee-3902abd37494.png)
![image](https://user-images.githubusercontent.com/55529455/163406106-32067ecf-0d83-48fd-87ff-43265424f9f8.png)
![image](https://user-images.githubusercontent.com/55529455/163406161-467e3393-b243-427f-a9ed-0258a8dc6dd6.png)
![image](https://user-images.githubusercontent.com/55529455/163406198-f1b50701-3534-4dfd-8acb-b835e658dc05.png)
![image](https://user-images.githubusercontent.com/55529455/163406230-967c3792-336b-4f5d-b3f0-73064fa2b7c4.png)
![image](https://user-images.githubusercontent.com/55529455/163406267-a8061f9f-f48a-4561-9662-62bfc2683f33.png)
![image](https://user-images.githubusercontent.com/55529455/163406316-e178e582-a22d-45de-ae60-bb6faac3f808.png)
![image](https://user-images.githubusercontent.com/55529455/163406345-595318bc-ecb7-48a2-b4e2-2d04edaca3aa.png)
![image](https://user-images.githubusercontent.com/55529455/163406465-9ff65050-f91c-4a63-87c1-7e34dc7345c8.png)
![image](https://user-images.githubusercontent.com/55529455/163406487-7266d7d4-a2d1-455c-a253-6196e7e9bc27.png)
![image](https://user-images.githubusercontent.com/55529455/163429730-56e531dd-269d-41ed-9784-afdc32fd3bb0.png)
![image](https://user-images.githubusercontent.com/55529455/163429761-6c845295-260d-4136-b640-75c762c5bf23.png)
![image](https://user-images.githubusercontent.com/55529455/163429787-52d8952d-9a76-41bd-8d6b-3b6b9a748d27.png)
![image](https://user-images.githubusercontent.com/55529455/163429815-4382032c-c95d-46c9-b323-ae638ad69c6f.png)
![image](https://user-images.githubusercontent.com/55529455/163429842-965f362c-f96e-4fc1-b3ca-b52412a753e4.png)
![image](https://user-images.githubusercontent.com/55529455/163429902-36c29ef6-68ab-4fe0-bfc1-65c55c91cf46.png)
![image](https://user-images.githubusercontent.com/55529455/163429928-9f181970-6e73-4263-ba72-3100b923f357.png)
![image](https://user-images.githubusercontent.com/55529455/163429968-d6d6f2a6-b942-4c31-9418-1870cce715d6.png)
![image](https://user-images.githubusercontent.com/55529455/163430006-3afee4f6-ba1e-419f-8c56-eed2efd345a5.png)
![image](https://user-images.githubusercontent.com/55529455/163430051-d9fe2a01-c6c5-44e6-8ea6-06189d766152.png)
![image](https://user-images.githubusercontent.com/55529455/163430079-7b50594b-eec8-4d80-9029-e265aab11a1e.png)
![image](https://user-images.githubusercontent.com/55529455/163430109-87d088f1-b170-41b6-8c53-f065db5e9ed3.png)
![image](https://user-images.githubusercontent.com/55529455/163430143-0b2d3365-001a-4e1f-a8b0-fe4a6ee3c87e.png)
![image](https://user-images.githubusercontent.com/55529455/163430161-0363fb91-ea79-4051-a9a1-ceffdbbe8bcb.png)
![image](https://user-images.githubusercontent.com/55529455/163430185-8e796f2f-c70f-4062-b8ea-9c8483e26c68.png)

> 은닉층 깊이에 따른 이점
> 지수의 표현
> 각 은닉층은 입력공간을 어디서 접을지 결정 -> 지수적으로 많은 선형적인 영역 조각들 -> 성능향상

> 다층 퍼셉트론 학습과정
> 입력층에서 은닉층을 통해 출력층으로 순방향 전파를 통해 오차 계산을 하고, 출력층에서 은닉층을 거쳐 입력층에 역방향 전파를 함.

> 예측단계
> 학습을 마친 후, 현장 설치하여 사용함.

![image](https://user-images.githubusercontent.com/55529455/163430448-f21de972-3e82-41ce-90fc-68479e1d322d.png)

> 다층 퍼셉트론 시각화
> https://playground.tensorflow.org

> 연산 복잡도 비교
> 오류역전파 = 전방 계산 대비 약 1.5 ~ 2배의 시간 소요 - 비교적 빠름 (연쇄법칙)
> c = 분류수, d = 특징 차원, p = 은닉층 자원, n = 훈련집합 크기, q = 세대 수
> 학습알고리즘은 오류 역전파를 반복하여 점근적 시감복잡도는 O((cp+dp)nq)

> 은닉층을 하나만 가진 다층 퍼셉트론은 범용 근사자 (Hornik 주장)
> 은닉노드가 충분히 많으면, 포화함수(활성함수)로 무엇을 사용하든, 표준 다층 퍼셉트론은 어떤 함수라도 원하는 정확도 만큼 근사화 할 수 있다.

> 순수한 최적화 알고리즘으로는 높은 성능 불가능
> 데이터 희소성, 잡음, 미숙한 싱경망 구조 등 이유
> 성능 향상을 위한 다양한 경험(휴리스틱)을 개발하고 공유함

> 신경망의 경험적 개발에서 중요 쟁점
> 아키텍처
> 초깃값
> 학습률
> 활성함수






























